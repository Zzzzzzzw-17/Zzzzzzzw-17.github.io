<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    <meta name="description" content="Hexo Theme Redefine">
    <meta name="author" content="Wei">
    
    <title>
        
            Analysis and Applications of Explanatory Signals from Prompt-Based Models |
        
        RabbitHole
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/Raven.svg">
    
<link rel="stylesheet" href="/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/css/v5-font-face.min.css">

    
<link rel="stylesheet" href="/css/duotone.min.css">

    
<link rel="stylesheet" href="/css/brands.min.css">

    
<link rel="stylesheet" href="/css/solid.min.css">

    
<link rel="stylesheet" href="/css/css2.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <script id="hexo-configurations">
    let REDEFINE = window.REDEFINE || {};
    REDEFINE.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.json"};
    REDEFINE.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#005080","avatar":"/images/Raven.svg","favicon":"/images/Raven.svg","article_img_align":"center","right_side_width":"210px","content_max_width":"1000px","nav_color":{"left":"#f78736","right":"#367df7","transparency":35},"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_image":{"light":"https://evan.beee.top/img/wallhaven-wqery6-light.webp","dark":"https://evan.beee.top/img/wallhaven-wqery6-dark.webp"},"title_color":{"light":"#fff","dark":"#d1d1b6"},"description":"遥天如有蓝鲸在，好送余音入远波"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true},"pjax":{"enable":true},"lazyload":{"enable":true},"version":"0.3.5"};
    REDEFINE.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">
    
    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                RabbitHole
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">
            <div class="article-title">
                <span class="title-hover-animation"><h1 style="font-size:2rem; font-weight: bold; margin: 10px 0;">Analysis and Applications of Explanatory Signals from Prompt-Based Models</h1></span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/Raven.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Wei</span>
                            
                                <span class="author-label">lol</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-duotone fa-pen-line"></i>&nbsp;
        <span class="pc">2022-12-29 15:09:15</span>
        <span class="mobile">2022-12-29 15:09</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fa-duotone fa-folder-open"></i></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/project/">project</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-duotone fa-tags"></i></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/prompting/">prompting</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/explainability/">explainability</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/low-resources/">low resources</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Explanatory signals, such as gradients, can provide information about the importance<br>of different input parts for the output of models, thus improving models’ interpretability.<br>In natural language processing (NLP), they reveal how different parts of texts, e.g.,<br>tokens, contribute to a prediction from black-box models, e.g., deep neural networks.<br>Current research on interpretability focuses on fine-tuned models: either extracting explanatory<br>signals from models for analysis or injecting them into models to improve task<br>performance or explanation quality. This poses a challenge to interpretability in lowresource<br>settings, where data are scarce to fine-tune models. We approach this challenge<br>by leveraging prompt-based models (PBMs) to provide explanations. A comprehensive<br>comparison between explanations obtained from PBMs and fine-tuned models is<br>conducted in various settings. More specifically, we explore the influence of training<br>size, prompting strategies, and explanation methods on the output explanations from<br>the perspective of plausibility, faithfulness, and a proxy task. The tasks and data sets<br>we explore are sentiment classification (Tweet Sentiment Extraction) and natural language<br>inference (e-SNLI). We find that PBMs generate more plausible explanations than<br>fine-tuned models in the low-resource setting. Additionally, among the three explanation<br>methods we examine, namely, attention, Integrated Gradients, and Shapley Value<br>Sampling, we observe Shapley Value Sampling consistently outperforms other methods<br>in plausibility and faithfulness. Apart from providing comparisons in various aspects,<br>we propose a novel method to improve task performance with explanations by enriching<br>the prompts of PBMs with explanatory signals. Our method achieves up to 3% of<br>performance gains compared with PBMs without explanations.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h2><p>The development of NLP models has brought notable improvements to various downstream tasks [<a class="link"   target="_blank" rel="noopener" href="https://aclanthology.org/N19-1423.pdf" >1<i class="fas fa-external-link-alt"></i></a>, <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165" >2<i class="fas fa-external-link-alt"></i></a>]. However, it comes at the expense of interpretability: as neural models become larger and more complex, it is difficult to disentangle their structures and explain how they generate their outputs. This is detrimental to fields where high-stakes decisions are made, such as criminal justice and finance [<a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.04840" >3<i class="fas fa-external-link-alt"></i></a>]. </p>
<p>In this work, we follow the definition from [<a class="link"   target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-319-98131-4_1" >4<i class="fas fa-external-link-alt"></i></a>],<br>that interpretability is “the ability [of a model] to explain or to present [its predictions]<br>in understandable terms to a human”. We treat explainability as the synonym of interpretability and use them interchangeably.</p>
<p>Given the status quo and why we need explanability in NLP, we move on to introduce the taxonomy of explanation methods and some common explantion methods. </p>
<h2 id="1-2-Taxonomy-of-explaination-methods"><a href="#1-2-Taxonomy-of-explaination-methods" class="headerlink" title="1.2 Taxonomy of explaination methods"></a>1.2 Taxonomy of explaination methods</h2><p><strong>Local vs. Global</strong>. One vital dimension to categorizing explanation methods is whether the<br>methods target interpreting specific instances or the whole model/reasoning mechanism<br>in general. The former includes SHAP ([<a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.07874" >5<i class="fas fa-external-link-alt"></i></a>]), and Lime ([<a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.04938" >6<i class="fas fa-external-link-alt"></i></a>] while the latter can be exemplified by decision trees.</p>
<p><strong>Post-hoc vs. Self-explaining</strong>. This dimension groups explanation methods by how they<br>are derived, i.e., applying post-hoc operations on a model or during the model’s prediction<br>process. The former technique is often model-agnostic, while the latter is deeply<br>integrated with the model’s working mechanism, exemplified by attention.</p>
<p><strong>Free texts vs. Token saliency</strong>. Another dimension distinguishes the way explanations<br>are shown: either by generating free texts as explanations or showing input importance, mostly tokens, by highlighting.</p>
<p align="center">
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:\Users\ASUS\Desktop\blog\source\_posts\Post-Asset-Folder\thesis\force_plot.png"
                      width="1500" height="120" 
                >
</p>

<p align="center">
Caption: Force plot generated by SHAP.
</p>

<h2 id="1-3-Common-explaination-methods"><a href="#1-3-Common-explaination-methods" class="headerlink" title="1.3 Common explaination methods"></a>1.3 Common explaination methods</h2><p>To make model behavior more explainable for the systems featuring unquantified variances,<br>intrinsically, one can develop white-box models that can be directly interpreted,<br>such as decision trees, or use parameters and weights as an indication<br>of feature importance, e.g., regression coefficients. However, the latter approach is under<br>debate whether they genuinely reflect models’ outcomes (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Just as Jacovi and Goldberg (2020) warn, a method being inherently interpretable is merely a claim that needs to be verified before it can<br>be trusted. </p>
<p>Another perspective is to provide post-hoc explanation methods1 that are<br>model agnostic, exemplified by gradients and its variations (Simonyan et al., 2014; Sundararajan<br>et al., 2017), LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee,<br>2017). These methods provide explanations by feature importance. In the case of NLP,<br>these features are mostly tokens. However, post-hoc methods are not without problems:<br>the final explanations are highly dependent on the models and data sets they test<br>(Atanasova et al., 2020; Ding and Koehn, 2021), and they can be easily fooled when<br>models do not faithfully show their predictions (Slack et al., 2020; Sinha et al., 2021).</p>
<h2 id="1-4-Current-explanability-reserach"><a href="#1-4-Current-explanability-reserach" class="headerlink" title="1.4 Current explanability reserach"></a>1.4 Current explanability reserach</h2><p>Studies focusing on model explanations with post-hoc attribution methods can be further<br>categorized into two directions: 1) Designing or analyzing explanation methods on<br>downstream tasks (Hao et al., 2021; Ding and Koehn, 2021). 2) Incorporating explanations<br>provided by attribution methods (or human-provided explanations) into model<br>training to enhance model performance and explanation quality (Zaidan et al., 2007;<br>Atanasova et al., 2022).</p>
<h1 id="2-Motivations-and-Goals-of-this-study"><a href="#2-Motivations-and-Goals-of-this-study" class="headerlink" title="2 Motivations and Goals of this study"></a>2 Motivations and Goals of this study</h1><h4 id="Motivation-To-address-low-resources-settings"><a href="#Motivation-To-address-low-resources-settings" class="headerlink" title="Motivation: To address low-resources settings"></a>Motivation: To address low-resources settings</h4><p>Current reserach heavily on the models trained/fine-tuned with rich datasets. However, not all tasks can be approached<br>by fine-tuning a pre-trained model or training a model from scratch. Moreover, obtaining annotated explanations is challenging both time-wise and resource-wise.</p>
<p>Considering low-resource settings where training data is scarce, it is challenging to develop<br>fine-tuned, or train-from-scratch models to perform downstream tasks, let alone<br>provide explanations for predictions. We think low-resource settings are worth paying<br>attention to and exploring because 1) principle perspective: social responsibility reasons (Ruder, 2020). We also want explanations in these settings. 2) practical perspective: explanations help understand models, by analyzing/incorporating explanations (into modells), we might gain more insights about the task and thus improve task performance, given limited data. </p>
<p>approach interpretability issues in low-resource settings, we propose combining promptbased<br>models (PBMs) with explanation methods. </p>
<h4 id="Challenge2-lacking-comparisions-between-attention-and-other-post-hoc-explanation-methods-with-PBM"><a href="#Challenge2-lacking-comparisions-between-attention-and-other-post-hoc-explanation-methods-with-PBM" class="headerlink" title="Challenge2: lacking comparisions between attention and other post-hoc explanation methods with PBM"></a>Challenge2: lacking comparisions between attention and other post-hoc explanation methods with PBM</h4><p>Besides the problem of lacking focus on low-resource settings in interpretability research,<br>another gap we notice is that there are rarely comprehensive comparisons between attention<br>and other attribution methods in terms of the explanations they give. Though some<br>works involve attention in the model’s interpretability (Sydorova et al., 2019; Pruthi<br>et al., 2022), they either focus on only one facet of the comparison or utilize the finetuned<br>models to derive explanations. We aim to compare explanations extracted by<br>attention and other attribution methods in both PBM and fine-tuned model settings.<br>We think such comparisons are crucial for providing direct observations of attention<br>performance against other methods and enabling more understanding of explanation<br>methods under different setups.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pretraining<br>of deep bidirectional transformers for language understanding. In Proceedings<br>of the 2019 Conference of the North American Chapter of the Association<br>for Computational Linguistics: Human Language Technologies, Volume 1 (Long<br>and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association<br>for Computational Linguistics. </p>
<p>[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla<br>Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,<br>Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon<br>Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher<br>Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack<br>Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario<br>Amodei. Language models are few-shot learners. In Proceedings of the 34th International<br>Conference on Neural Information Processing Systems, NIPS’20, Red Hook,<br>NY, USA, 2020. Curran Associates Inc. </p>
<p>[3] Andreas Madsen, Siva Reddy, and Sarath Chandar. Post-hoc interpretability for neural<br>nlp: A survey. ACM Comput. Surv., jul 2022.</p>
<p>[4] Finale Doshi-Velez and Been Kim. Considerations for Evaluation and Generalization in<br>Interpretable Machine Learning, pages 3–17. Springer International Publishing, Cham,<br>2018.</p>
<p>[5] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions.<br>In Proceedings of the 31st International Conference on Neural Information Processing<br>Systems, NIPS’17, page 4768–4777, Red Hook, NY, USA, 2017. Curran Associates<br>Inc.</p>
<p>[6] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”:<br>Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD<br>International Conference on Knowledge Discovery and Data Mining, 2016.</p>
<h1 id="What-Do-Vision-and-Language-Models-NOT-learn"><a href="#What-Do-Vision-and-Language-Models-NOT-learn" class="headerlink" title="What Do Vision and Language Models NOT learn"></a>What Do Vision and Language Models NOT learn</h1><p>This is a summary of the project from the course <em>Lanugage and Vision</em> at the University of Stuttgart.</p>
<p>UPDATE: our paper is accepted in the student session at KONVENS 2022</p>
<h2 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h2><ol>
<li>Intro to pretrained LV models</li>
<li>Intro to this project</li>
<li>Methodology</li>
<li>Experiments and results</li>
<li>Future work</li>
</ol>
<h2 id="1-Intro-to-pretrained-LV-models"><a href="#1-Intro-to-pretrained-LV-models" class="headerlink" title="1. Intro to pretrained LV models"></a>1. Intro to pretrained LV models</h2><p>good resources to start:</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2022-06-09-vlm/" >https://lilianweng.github.io/posts/2022-06-09-vlm/<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.10936.pdf" >https://arxiv.org/pdf/2202.10936.pdf<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="2-Intro-to-this-project"><a href="#2-Intro-to-this-project" class="headerlink" title="2. Intro to this project"></a>2. Intro to this project</h2><p>Pretrained LV models seems to work well on various tasks, but blindly applying them without understanding what knowledge they actually learn is dangerous.</p>
<p> Therefore, we try to answer the question: <strong>what do the pretrained LV models actually learn?</strong>  Specifically, we want to know if LV models encode information about:  <strong>color</strong>, <strong>number</strong>, <strong>nouns</strong>, <strong>verbs</strong> and <strong>syntax</strong>. </p>
<h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><p>The main idea of our method is that  <strong>changing key information in a caption should confuse the models when the models attend to both the visual and texual information</strong>. </p>
<p>For instance, the below caption and image are perfectly aligned. But if I change the word “cat” in the caption into “dog”. You immediately notice the image and caption do not align any more, because you have the knowledge of what a cat looks like.</p>
<p align="center">
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://i0.wp.com/theverybesttop10.com/wp-content/uploads/2016/09/Top-10-Fit-Active-Cats-Who-Love-Sports-10.jpg?resize=768,551&ssl=1"
                      width="300" height="150" 
                >
</p>

<p align="center">
Caption: A cat is playing with a ball.
</p>


<p>Based on this idea, we create probes (“distorted” captions),  feed them into the models with images and compare model’s confidence in the alignment between the image and captions of the original and probe cases. In the following, I will explain:</p>
<ul>
<li>what are these probes and how to create them?</li>
<li>which dataset and models did we use?</li>
<li>what does the “model’s confidence” mean? how to evaluate that?</li>
</ul>
<h3 id="3-1-Creating-probes"><a href="#3-1-Creating-probes" class="headerlink" title="3.1 Creating probes"></a>3.1 Creating probes</h3><h4 id="3-1-1-Color"><a href="#3-1-1-Color" class="headerlink" title="3.1.1 Color"></a>3.1.1 Color</h4><p><strong>Goal</strong>:</p>
<ol>
<li>Are the models able to tell whether a color property of an object in the caption matches the color of the respective object in the image?</li>
<li>When the models fail to detect the mismatch, is this caused by the learned priors (dataset or linguistic priors)?</li>
</ol>
<p><strong>How</strong>:</p>
<ol>
<li>blind sampled colors: blind sampling is done by sampling from a predefined set of colors randomly, and substituting the original color in the caption with the sampled one.</li>
<li>controlled sampled colors: controlled sampling is analogous to the blind sampling, however, here, we aggregate all the color attributes of<br>the objects in the dataset to create a plausible set of colors that each object can have.</li>
</ol>
<p>In the caption generation, we then sample from the respective set of plausible colors of the object, and substitute the original color with this sampled one.</p>
<h4 id="3-1-2-Number"><a href="#3-1-2-Number" class="headerlink" title="3.1.2 Number"></a>3.1.2 Number</h4><p><strong>Goal</strong>: examining models’ ability to exact counting of objects.</p>
<p><strong>How</strong>: we do not probe for groups of objects of unspecified numbers (e.g. many, several,multiple). The caption generation of number probes follow analogously to that of the blind color probes.</p>
<h4 id="3-1-3-Nouns"><a href="#3-1-3-Nouns" class="headerlink" title="3.1.3 Nouns"></a>3.1.3 Nouns</h4><p><strong>Goal</strong></p>
<ol>
<li>determine whether a model is able to detect that a noun in the caption does not match the respective object in the image.</li>
<li>determine whether a model has a tendency of preferring a more or a less general concept.</li>
</ol>
<p><strong>How</strong></p>
<ol>
<li>Noun probing:  in each caption we randomly select one of the nouns and replace it with another noun, randomly chosen out of 10 most similar words according to a Gensim model. For instance, donut is replaced with bagel, and car is replaced with motorcycle.</li>
<li>we replace a random noun in a caption with its hypernym from WordNet: dog is replaced with domesticated animal, and hat is<br> replaced with headgear. Important to note here is that such a replacement might make a caption less specific, however does not create a mismatch between the image and the caption.</li>
</ol>
<h4 id="3-1-4-Verbs"><a href="#3-1-4-Verbs" class="headerlink" title="3.1.4 Verbs"></a>3.1.4 Verbs</h4><p><strong>Goal</strong>:  test models’ ability of correctly identifying verbs denoting actions with two types of replacement: physical and relational.</p>
<p><strong>How</strong></p>
<ol>
<li>physical probes: we replace a verb with its semantically similar counterpart.</li>
<li>relational probes: we extract the subject-verb-object (SVO) pair from a caption and replace the verb with a semantically different verb that fits in the SVO template: &lt;girl, climbing, stairs&gt; is changed to &lt;girl, cleaning, stairs&gt;.<h4 id="3-1-5-Syntax"><a href="#3-1-5-Syntax" class="headerlink" title="3.1.5. Syntax"></a>3.1.5. Syntax</h4></li>
</ol>
<p><strong>Goal</strong></p>
<ol>
<li>Is the compositionality of concepts preserved in the pretrained models, or are the models learning concepts non-compositionally: e.g. is wooden building treated as a combined concept or as two separate concepts (wooden and building)?</li>
<li>Is there an observable tendency of preferring more/less detailed descriptions: e.g. does removing wooden from wooden building result in a significant change in the perceived similarity of the image and caption pair?</li>
</ol>
<p><strong>How</strong></p>
<p>we select captions that contain two nouns (objects in the image) with at least one of them having an adjective modifier (property of the object), and create probes for each qualified caption by swapping their properties (adj-swap). To answer the second question, we create probes by removing the adjective modifiers of a noun (no-adj).</p>
<h3 id="3-2-Dataset-and-models"><a href="#3-2-Dataset-and-models" class="headerlink" title="3.2 Dataset and models"></a>3.2 Dataset and models</h3><ul>
<li>Flickr8k dataset consisting of 8,000 images that are each<br>paired with five different captions</li>
<li>CLIP and VisualBert</li>
</ul>
<h3 id="3-3-Evaluations"><a href="#3-3-Evaluations" class="headerlink" title="3.3 Evaluations"></a>3.3 Evaluations</h3><p>we extract the image-text similarity scores for the label 1 (match case) from the models for the original and modified captions, and feed these into a softmax classifier to obtain our results. </p>
<p>We report on the accuracies of these preferences and the averaged similarity scores. </p>
<h2 id="4-Result-and-implications"><a href="#4-Result-and-implications" class="headerlink" title="4. Result and implications"></a>4. Result and implications</h2><table>
<thead>
<tr>
<th>Probe</th>
<th>num instance</th>
<th>CLIP</th>
<th>VisualBERT</th>
</tr>
</thead>
<tbody><tr>
<td>ctrl color</td>
<td>3120</td>
<td>0.80</td>
<td>0.59</td>
</tr>
<tr>
<td>blind color</td>
<td>14255</td>
<td>0.86</td>
<td>0.54</td>
</tr>
<tr>
<td>number</td>
<td>8138</td>
<td>0.85</td>
<td>0.72</td>
</tr>
<tr>
<td>syntax adj-swap</td>
<td>4711</td>
<td>0.68</td>
<td>0.56</td>
</tr>
<tr>
<td>syntax no-adj</td>
<td>4711</td>
<td>0.79</td>
<td>0.43</td>
</tr>
<tr>
<td>noun</td>
<td>39856</td>
<td>0.80</td>
<td>0.58</td>
</tr>
<tr>
<td>hypernym</td>
<td>40283</td>
<td>0.69</td>
<td>0.59</td>
</tr>
<tr>
<td>verb physical</td>
<td>107</td>
<td>0.71</td>
<td>0.62</td>
</tr>
<tr>
<td>verb relational</td>
<td>107</td>
<td>0.89</td>
<td>0.59</td>
</tr>
</tbody></table>
<ol>
<li>CLIP performs much better than Visualbert</li>
<li>as expected, blind color is easier than ctrl color, there might be a learned prior of color and nouns.</li>
<li>we could imagine that swapping adj should be easier than deleting adj, because the latter case does not necessarily result in wrong captions. However, we found CLIP thinks the opposite. This might suggest having attributes is more important for matching image and caption than attaching the right adj to the right nounds. The model might still learn concept individually.</li>
<li>as expected, hypernym is more difficult.</li>
</ol>
<h2 id="5-Future-works"><a href="#5-Future-works" class="headerlink" title="5. Future works"></a>5. Future works</h2><ul>
<li>incorporate more models</li>
</ul>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li>Post title：Analysis and Applications of Explanatory Signals from Prompt-Based Models</li>
        <li>Post author：Wei</li>
        <li>Create time：2022-12-29 15:09:15</li>
        <li>
            Post link：https://zzzzzzzw-17.github.io/2022/12/29/vision/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/prompting/">#prompting</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/explainability/">#explainability</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/low-resources/">#low resources</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2022/12/29/thesis/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">Analysis and Applications of Explanatory Signals from Prompt-Based Models</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                <i class="fas fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            

            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            

        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div style="font-size: 1.3rem;margin-top: 0; margin-bottom: 0.8rem; transition-duration: 0.1s;"><i class="fa-solid fa-list"></i> <strong>Contents</strong></div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Abstract"><span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Background"><span class="nav-text">1.1 Background</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Taxonomy-of-explaination-methods"><span class="nav-text">1.2 Taxonomy of explaination methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Common-explaination-methods"><span class="nav-text">1.3 Common explaination methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-Current-explanability-reserach"><span class="nav-text">1.4 Current explanability reserach</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Motivations-and-Goals-of-this-study"><span class="nav-text">2 Motivations and Goals of this study</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivation-To-address-low-resources-settings"><span class="nav-text">Motivation: To address low-resources settings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Challenge2-lacking-comparisions-between-attention-and-other-post-hoc-explanation-methods-with-PBM"><span class="nav-text">Challenge2: lacking comparisions between attention and other post-hoc explanation methods with PBM</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-Do-Vision-and-Language-Models-NOT-learn"><span class="nav-text">What Do Vision and Language Models NOT learn</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Contents"><span class="nav-text">Contents</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Intro-to-pretrained-LV-models"><span class="nav-text">1. Intro to pretrained LV models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Intro-to-this-project"><span class="nav-text">2. Intro to this project</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Methodology"><span class="nav-text">3. Methodology</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Creating-probes"><span class="nav-text">3.1 Creating probes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-Color"><span class="nav-text">3.1.1 Color</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-Number"><span class="nav-text">3.1.2 Number</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-Nouns"><span class="nav-text">3.1.3 Nouns</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-4-Verbs"><span class="nav-text">3.1.4 Verbs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-5-Syntax"><span class="nav-text">3.1.5. Syntax</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Dataset-and-models"><span class="nav-text">3.2 Dataset and models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Evaluations"><span class="nav-text">3.3 Evaluations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Result-and-implications"><span class="nav-text">4. Result and implications</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Future-works"><span class="nav-text">5. Future works</span></a></li></ol></li></ol>
    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>



        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2023&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Wei</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v0.3.5</a>
        </div>
        
        
    </div>
    <link rel="stylesheet" href="//evan.beee.top/css/waline.css"/>
    <script src="//evan.beee.top/js/waline.js"></script>
    
<link rel="stylesheet" href="/css/regular.min.css">

</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fa-duotone fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-duotone fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>




    
<script src="/js/lazyload.js"></script>



<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            REDEFINE.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            REDEFINE.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            REDEFINE.refresh();
        });
    });
</script>



</body>
</html>
