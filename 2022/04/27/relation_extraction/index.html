<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    <meta name="description" content="Hexo Theme Redefine">
    <meta name="author" content="Wei">
    
    <title>
        
            Enriched Attention with Entity Masking |
        
        RabbitHole
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/Raven.svg">
    
<link rel="stylesheet" href="/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/css/v5-font-face.min.css">

    
<link rel="stylesheet" href="/css/duotone.min.css">

    
<link rel="stylesheet" href="/css/brands.min.css">

    
<link rel="stylesheet" href="/css/solid.min.css">

    
<link rel="stylesheet" href="/css/css2.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <script id="hexo-configurations">
    let REDEFINE = window.REDEFINE || {};
    REDEFINE.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.json"};
    REDEFINE.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#005080","avatar":"/images/Raven.svg","favicon":"/images/Raven.svg","article_img_align":"center","right_side_width":"210px","content_max_width":"1000px","nav_color":{"left":"#f78736","right":"#367df7","transparency":35},"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_image":{"light":"https://evan.beee.top/img/wallhaven-wqery6-light.webp","dark":"https://evan.beee.top/img/wallhaven-wqery6-dark.webp"},"title_color":{"light":"#fff","dark":"#d1d1b6"},"description":"遥天定有蓝鲸在，好送余音入远波"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true},"pjax":{"enable":true},"lazyload":{"enable":true},"version":"0.3.5"};
    REDEFINE.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">
    
    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                RabbitHole
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">
            <div class="article-title">
                <span class="title-hover-animation"><h1 style="font-size:2rem; font-weight: bold; margin: 10px 0;">Enriched Attention with Entity Masking</h1></span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/Raven.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Wei</span>
                            
                                <span class="author-label">lol</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-duotone fa-pen-line"></i>&nbsp;
        <span class="pc">2022-04-27 15:09:15</span>
        <span class="mobile">2022-04-27 15:09</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fa-duotone fa-folder-open"></i></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/work/">work</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-duotone fa-tags"></i></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/embeddings/">embeddings</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/relation-classification/">relation classification</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="Enriched-Attention-with-Entity-Masking"><a href="#Enriched-Attention-with-Entity-Masking" class="headerlink" title="Enriched Attention with Entity Masking"></a>Enriched Attention with Entity Masking</h1><p>This is a summary of a relation classification task with enriched attention. Please refer to <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.10899" >this paper<i class="fas fa-external-link-alt"></i></a> for more information.</p>
<p>Contents covered are listed as follows: </p>
<ul>
<li>Intro to the relation extraction task </li>
<li>Related works</li>
<li>Intro to Meta-Embeddings</li>
<li>Experiments</li>
<li>Results</li>
<li>Take-away-messages</li>
</ul>
<h2 id="1-Relation-extraction"><a href="#1-Relation-extraction" class="headerlink" title="1. Relation extraction"></a>1. Relation extraction</h2><p><strong>1.1 Task definition</strong>: </p>
<p>Given a sentence and two arguments, predict the relationship of the two arguments. </p>
<blockquote>
<p>E.g., <code>Barack Obama Sr.</code>, the father of <code>Barack Obama</code>, was born is<br><code>1936</code> and married his first wife <code>Kezia</code> at the age of 18.</p>
</blockquote>
<p>In this sentence, we can have four arguments and many relation pairs: <code>Barack Obama Sr.</code> and <code>Barack Obama</code> have the relationship of <code>is_father_of</code>, <code>Barack Obama Sr.</code> and <code>1936</code> can have the relationship of <code>born_in</code> and <code>Barack Obama Sr.</code> and <code>Kezia</code> the relationship <code>husband_and_wife</code>. These relationships are defined by researchers.</p>
<p><strong>1.2 Usage</strong></p>
<p>knowledge graph population: converting unstructured data in structured data.<br>For instance, if we have a document, we can identity all potential arguments (NER detection) and get their relationship. We can append the relation information into the knowledge graph</p>
<p><strong>1.3 Challenges</strong></p>
<p>Sentence can be long, the distance between intersted arguments can be long, there can be distractors between the intersted arguments.</p>
<p><strong>1.4 Datasets</strong></p>
<p>We will focus on the <a class="link"   target="_blank" rel="noopener" href="https://aclanthology.org/D17-1004.pdf" >Tacred<i class="fas fa-external-link-alt"></i></a> dataset only in this post.</p>
<h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><ul>
<li>Enriched attention</li>
<li>TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task</li>
<li>Relation Classification with Entity Type Restriction</li>
<li>An Improved Baseline for Sentence-level Relation Extraction<br>take-away-messages: entity nanmes hepls/ better ways to encode entity information into the texts/ unmasking does not lead to overfitting</li>
<li> A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models</li>
</ul>
<h2 id="3-Meta-embeddings"><a href="#3-Meta-embeddings" class="headerlink" title="3. Meta-embeddings"></a>3. Meta-embeddings</h2><p>The basic idea is that we can combine different types of embeddings to have better representations for tokens. More information can be found in <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.12305.pdf" >this paper<i class="fas fa-external-link-alt"></i></a>.</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p> Here is a summary table of the configurations that I tried. You can find the explanations below.</p>
<table>
<thead>
<tr>
<th>embeddings</th>
<th>encoder</th>
<th>masking</th>
<th>attention</th>
<th>additional_features</th>
<th>note</th>
</tr>
</thead>
<tbody><tr>
<td>glove/bpe/transformer/meta</td>
<td>bi-lstm</td>
<td>entity mark</td>
<td>true / false</td>
<td>global, local dependency, entity types</td>
<td>default</td>
</tr>
<tr>
<td>glove/bpe/transformer/meta</td>
<td>bi-lstm</td>
<td>all entity masking strategy except entity mark</td>
<td>false</td>
<td>none</td>
<td>choose masking strategy</td>
</tr>
<tr>
<td>transformer</td>
<td>transformer</td>
<td>all-masking</td>
<td>false</td>
<td>none</td>
<td>transformer_only</td>
</tr>
</tbody></table>
<h3 id="Explanations"><a href="#Explanations" class="headerlink" title="Explanations"></a>Explanations</h3><ol>
<li><p><strong>Embeddings</strong>: different embeddings were tested, namely glove-300d, bpe embeddings, transformer embeddings (roberta and xlm-roberta) and combinations of them (suggested by the option <code>meta</code>). </p>
</li>
<li><p><strong>Encoder</strong>: two types of encoder were examined, namely bi-lstm encoder and transformer encoder. The former is from <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.10899" >this paper<i class="fas fa-external-link-alt"></i></a>  and the latter <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.01373" >this<i class="fas fa-external-link-alt"></i></a> .</p>
</li>
<li><p><strong>Masking</strong>: five different entity masking strategies were tested. They are:</p>
<p>  🎀 The original sentence is: Bill was born in Seattle. </p>
<ul>
<li><p><strong>Entity mask</strong>: [SUBJ-PERSON] was born in [OBJ-CITY]. </p>
</li>
<li><p><strong>Entity marker</strong>:  [E1] Bill [/E1] was born in [E2] Seattle [/E2]. </p>
</li>
<li><p><strong>Entity marker (punct)</strong>:  @ Bill @ was born in # Seattle #. </p>
</li>
<li><p><strong>Typed entity marker</strong>: 〈S:PERSON〉 Bill 〈/S:PERSON〉 was born in 〈O:CITY〉 Seattle 〈/O:CITY〉. </p>
</li>
<li><p><strong>Typed entity marker (punct)</strong>: @ * person * Bill @ was born in # ∧ city ∧ Seattle #. </p>
</li>
</ul>
</li>
<li><p><strong>Attention and additional features</strong>: these are exclusive to the lstm encoder. For more information, please refer to  <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.10899" >this paper<i class="fas fa-external-link-alt"></i></a> .</p>
</li>
</ol>
<h2 id="5-Results-on-Tacred-dataset"><a href="#5-Results-on-Tacred-dataset" class="headerlink" title="5. Results on Tacred dataset"></a>5. Results on Tacred dataset</h2><p>Notation</p>
<ol>
<li><strong>default</strong> : BILSTM model, no attention, no additional features, use only GLOVE embeddings.</li>
<li><strong>default + xx</strong>: additional settings added on the default mode.</li>
<li><strong>default * xx</strong> : change element in default setting, for instance, embedding method .</li>
<li><strong>transformer</strong>: using roberta-base model as encoder, no additional features, use entity mask.</li>
</ol>
<table>
<thead>
<tr>
<th>setting</th>
<th>model</th>
<th>dev f1</th>
<th>test f1</th>
<th>setting details</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>default</td>
<td>0.6473</td>
<td>0.6199</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>default + attn</td>
<td>0.6455</td>
<td>0.6414</td>
<td>with attn=true, global, local and entity info</td>
</tr>
<tr>
<td>3</td>
<td>default * masking</td>
<td>0.6208</td>
<td>0.5977</td>
<td>change masking from entity mask to type entity marker punct</td>
</tr>
<tr>
<td>4</td>
<td>default * embeddings + attn</td>
<td>0.6540</td>
<td>0.6620</td>
<td>{emb: bpe and GLOVE}, {attn: true}, {local, global, entity type: true}</td>
</tr>
<tr>
<td>5</td>
<td>default * embeddings</td>
<td>0.6544</td>
<td>0.6388</td>
<td>{emb: bpe and GLOVE}</td>
</tr>
<tr>
<td>6</td>
<td>default * embeddings * masking</td>
<td>0.6295</td>
<td>0.5951</td>
<td>{emb: bpe and GLOVE}, {masking: entity marker}</td>
</tr>
<tr>
<td>7</td>
<td>default * embeddings * masking</td>
<td>0.6201</td>
<td>0.5946</td>
<td>{emb: bpe and GLOVE}, {masking: entity marker (punct)}</td>
</tr>
<tr>
<td>8</td>
<td>default * embeddings * masking</td>
<td>0.6545</td>
<td>0.6329</td>
<td>{emb: bpe and GLOVE}, {masking: typed entity marker}</td>
</tr>
<tr>
<td>9</td>
<td>default * embeddings * masking</td>
<td>0.6566</td>
<td>0.6397</td>
<td>{emb: bpe and GLOVE}, {masking: typed entity marker (punct)}</td>
</tr>
<tr>
<td>10</td>
<td>transformer</td>
<td>0.6971</td>
<td>0.7030</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>transformer * masking</td>
<td>0.7113</td>
<td>0.7063</td>
<td>change masking from entity mask to type entity marker punct</td>
</tr>
</tbody></table>
<h2 id="6-Take-away-messages"><a href="#6-Take-away-messages" class="headerlink" title="6. Take-away-messages"></a>6. Take-away-messages</h2><ol>
<li>transformer encoder performs better than bi-lstm.</li>
<li>comparing different masking strategies (setting 5-9),  we may conclude incorporating entity type information into text is beneficial.</li>
<li>attention and additional features (setting1-2) helps lstm encoder for this task.</li>
<li>a combination of bpe and GLOVE embedding seems to work better than using GLOVE only. (setting1 and setting5)</li>
</ol>
<hr>
<p>This work is fully supported by Bosch. </p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li>Post title：Enriched Attention with Entity Masking</li>
        <li>Post author：Wei</li>
        <li>Create time：2022-04-27 15:09:15</li>
        <li>
            Post link：https://zzzzzzzw-17.github.io/2022/04/27/relation_extraction/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/embeddings/">#embeddings</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/relation-classification/">#relation classification</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2022/04/27/vision/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">Probing into Language and Vision model</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                <i class="fas fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            

            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            

        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div style="font-size: 1.3rem;margin-top: 0; margin-bottom: 0.8rem; transition-duration: 0.1s;"><i class="fa-solid fa-list"></i> <strong>Contents</strong></div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Enriched-Attention-with-Entity-Masking"><span class="nav-text">Enriched Attention with Entity Masking</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Relation-extraction"><span class="nav-text">1. Relation extraction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-work"><span class="nav-text">2. Related work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Meta-embeddings"><span class="nav-text">3. Meta-embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Experiments"><span class="nav-text">4. Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Explanations"><span class="nav-text">Explanations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Results-on-Tacred-dataset"><span class="nav-text">5. Results on Tacred dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Take-away-messages"><span class="nav-text">6. Take-away-messages</span></a></li></ol></li></ol>
    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>



        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2022&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Wei</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v0.3.5</a>
        </div>
        
        
    </div>
    <link rel="stylesheet" href="//evan.beee.top/css/waline.css"/>
    <script src="//evan.beee.top/js/waline.js"></script>
    
<link rel="stylesheet" href="/css/regular.min.css">

</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fa-duotone fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-duotone fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>




    
<script src="/js/lazyload.js"></script>



<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            REDEFINE.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            REDEFINE.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            REDEFINE.refresh();
        });
    });
</script>



</body>
</html>
