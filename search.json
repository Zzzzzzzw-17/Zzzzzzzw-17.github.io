[{"title":"Enriched Attention with Entity Masking","url":"/2022/04/27/relation_extraction/","content":"Enriched Attention with Entity MaskingThis is a summary of a relation classification task with enriched attention. Please refer to this paper for more information.\nContents covered are listed as follows: \n\nIntro to the relation extraction task \nRelated works\nIntro to Meta-Embeddings\nExperiments\nResults\nTake-away-messages\n\n1. Relation extraction1.1 Task definition: \nGiven a sentence and two arguments, predict the relationship of the two arguments. \n\nE.g., Barack Obama Sr., the father of Barack Obama, was born is1936 and married his first wife Kezia at the age of 18.\n\nIn this sentence, we can have four arguments and many relation pairs: Barack Obama Sr. and Barack Obama have the relationship of is_father_of, Barack Obama Sr. and 1936 can have the relationship of born_in and Barack Obama Sr. and Kezia the relationship husband_and_wife. These relationships are defined by researchers.\n1.2 Usage\nknowledge graph population: converting unstructured data in structured data.For instance, if we have a document, we can identity all potential arguments (NER detection) and get their relationship. We can append the relation information into the knowledge graph\n1.3 Challenges\nSentence can be long, the distance between intersted arguments can be long, there can be distractors between the intersted arguments.\n1.4 Datasets\nWe will focus on the Tacred dataset only in this post.\n2. Related work\nEnriched attention\nTACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task\nRelation Classification with Entity Type Restriction\nAn Improved Baseline for Sentence-level Relation Extractiontake-away-messages: entity nanmes hepls/ better ways to encode entity information into the texts/ unmasking does not lead to overfitting\n A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models\n\n3. Meta-embeddingsThe basic idea is that we can combine different types of embeddings to have better representations for tokens. More information can be found in this paper.\n4. Experiments Here is a summary table of the configurations that I tried. You can find the explanations below.\n\n\n\nembeddings\nencoder\nmasking\nattention\nadditional_features\nnote\n\n\n\nglove/bpe/transformer/meta\nbi-lstm\nentity mark\ntrue / false\nglobal, local dependency, entity types\ndefault\n\n\nglove/bpe/transformer/meta\nbi-lstm\nall entity masking strategy except entity mark\nfalse\nnone\nchoose masking strategy\n\n\ntransformer\ntransformer\nall-masking\nfalse\nnone\ntransformer_only\n\n\nExplanations\nEmbeddings: different embeddings were tested, namely glove-300d, bpe embeddings, transformer embeddings (roberta and xlm-roberta) and combinations of them (suggested by the option meta). \n\nEncoder: two types of encoder were examined, namely bi-lstm encoder and transformer encoder. The former is from this paper  and the latter this .\n\nMasking: five different entity masking strategies were tested. They are:\n  üéÄ The original sentence is: Bill was born in Seattle. \n\nEntity mask: [SUBJ-PERSON] was born in [OBJ-CITY]. \n\nEntity marker:  [E1] Bill [/E1] was born in [E2] Seattle [/E2]. \n\nEntity marker (punct):  @ Bill @ was born in # Seattle #. \n\nTyped entity marker: „ÄàS:PERSON„Äâ Bill „Äà/S:PERSON„Äâ was born in „ÄàO:CITY„Äâ Seattle „Äà/O:CITY„Äâ. \n\nTyped entity marker (punct): @ * person * Bill @ was born in # ‚àß city ‚àß Seattle #. \n\n\n\nAttention and additional features: these are exclusive to the lstm encoder. For more information, please refer to  this paper .\n\n\n5. Results on Tacred datasetNotation\n\ndefault : BILSTM model, no attention, no additional features, use only GLOVE embeddings.\ndefault + xx: additional settings added on the default mode.\ndefault * xx : change element in default setting, for instance, embedding method .\ntransformer: using roberta-base model as encoder, no additional features, use entity mask.\n\n\n\n\nsetting\nmodel\ndev f1\ntest f1\nsetting details\n\n\n\n1\ndefault\n0.6473\n0.6199\n\n\n\n2\ndefault + attn\n0.6455\n0.6414\nwith attn=true, global, local and entity info\n\n\n3\ndefault * masking\n0.6208\n0.5977\nchange masking from entity mask to type entity marker punct\n\n\n4\ndefault * embeddings + attn\n0.6540\n0.6620\n{emb: bpe and GLOVE}, {attn: true}, {local, global, entity type: true}\n\n\n5\ndefault * embeddings\n0.6544\n0.6388\n{emb: bpe and GLOVE}\n\n\n6\ndefault * embeddings * masking\n0.6295\n0.5951\n{emb: bpe and GLOVE}, {masking: entity marker}\n\n\n7\ndefault * embeddings * masking\n0.6201\n0.5946\n{emb: bpe and GLOVE}, {masking: entity marker (punct)}\n\n\n8\ndefault * embeddings * masking\n0.6545\n0.6329\n{emb: bpe and GLOVE}, {masking: typed entity marker}\n\n\n9\ndefault * embeddings * masking\n0.6566\n0.6397\n{emb: bpe and GLOVE}, {masking: typed entity marker (punct)}\n\n\n10\ntransformer\n0.6971\n0.7030\n\n\n\n11\ntransformer * masking\n0.7113\n0.7063\nchange masking from entity mask to type entity marker punct\n\n\n6. Take-away-messages\ntransformer encoder performs better than bi-lstm.\ncomparing different masking strategies (setting 5-9),  we may conclude incorporating entity type information into text is beneficial.\nattention and additional features (setting1-2) helps lstm encoder for this task.\na combination of bpe and GLOVE embedding seems to work better than using GLOVE only. (setting1 and setting5)\n\n\nThis work is fully supported by Bosch. \n","categories":["work"],"tags":["embeddings","relation classification"]},{"title":"Hexo error archive","url":"/2022/04/27/error-archive/","content":"\nWhen compiling markdown file\n\nerr: YAMLException: end of the stream or a document separator is expected at line 6, column 39:\n\n\n\nI browse through many blogs, some say it is a problem of not using space property, i.e., you should add an empty space   after :.But this is not the problem for my case, the problem is I did not add the title format into my markdown file.\n\n\ntitle: Hexo error archivedate: 2022-04-27 12:48:43tags:\n\n\n"},{"title":"Analysis and Applications of Explanatory Signals from Prompt-Based Models","url":"/2022/12/29/vision/","content":"AbstractExplanatory signals, such as gradients, can provide information about the importanceof different input parts for the output of models, thus improving models‚Äô interpretability.In natural language processing (NLP), they reveal how different parts of texts, e.g.,tokens, contribute to a prediction from black-box models, e.g., deep neural networks.Current research on interpretability focuses on fine-tuned models: either extracting explanatorysignals from models for analysis or injecting them into models to improve taskperformance or explanation quality. This poses a challenge to interpretability in lowresourcesettings, where data are scarce to fine-tune models. We approach this challengeby leveraging prompt-based models (PBMs) to provide explanations. A comprehensivecomparison between explanations obtained from PBMs and fine-tuned models isconducted in various settings. More specifically, we explore the influence of trainingsize, prompting strategies, and explanation methods on the output explanations fromthe perspective of plausibility, faithfulness, and a proxy task. The tasks and data setswe explore are sentiment classification (Tweet Sentiment Extraction) and natural languageinference (e-SNLI). We find that PBMs generate more plausible explanations thanfine-tuned models in the low-resource setting. Additionally, among the three explanationmethods we examine, namely, attention, Integrated Gradients, and Shapley ValueSampling, we observe Shapley Value Sampling consistently outperforms other methodsin plausibility and faithfulness. Apart from providing comparisons in various aspects,we propose a novel method to improve task performance with explanations by enrichingthe prompts of PBMs with explanatory signals. Our method achieves up to 3% ofperformance gains compared with PBMs without explanations.\n1. Introduction1.1 BackgroundThe development of NLP models has brought notable improvements to various downstream tasks [1, 2]. However, it comes at the expense of interpretability: as neural models become larger and more complex, it is difficult to disentangle their structures and explain how they generate their outputs. This is detrimental to fields where high-stakes decisions are made, such as criminal justice and finance [3]. \nIn this work, we follow the definition from [4],that interpretability is ‚Äúthe ability [of a model] to explain or to present [its predictions]in understandable terms to a human‚Äù. We treat explainability as the synonym of interpretability and use them interchangeably.\nGiven the status quo and why we need explanability in NLP, we move on to introduce the taxonomy of explanation methods and some common explantion methods. \n1.2 Taxonomy of explaination methodsLocal vs. Global. One vital dimension to categorizing explanation methods is whether themethods target interpreting specific instances or the whole model/reasoning mechanismin general. The former includes SHAP ([5]), and Lime ([6] while the latter can be exemplified by decision trees.\nPost-hoc vs. Self-explaining. This dimension groups explanation methods by how theyare derived, i.e., applying post-hoc operations on a model or during the model‚Äôs predictionprocess. The former technique is often model-agnostic, while the latter is deeplyintegrated with the model‚Äôs working mechanism, exemplified by attention.\nFree texts vs. Token saliency. Another dimension distinguishes the way explanationsare shown: either by generating free texts as explanations or showing input importance, mostly tokens, by highlighting.\n\n\n\n\n\nCaption: Force plot generated by SHAP.\n\n\n1.3 Common explaination methodsTo make model behavior more explainable for the systems featuring unquantified variances,intrinsically, one can develop white-box models that can be directly interpreted,such as decision trees, or use parameters and weights as an indicationof feature importance, e.g., regression coefficients. However, the latter approach is underdebate whether they genuinely reflect models‚Äô outcomes (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Just as Jacovi and Goldberg (2020) warn, a method being inherently interpretable is merely a claim that needs to be verified before it canbe trusted. \nAnother perspective is to provide post-hoc explanation methods1 that aremodel agnostic, exemplified by gradients and its variations (Simonyan et al., 2014; Sundararajanet al., 2017), LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee,2017). These methods provide explanations by feature importance. In the case of NLP,these features are mostly tokens. However, post-hoc methods are not without problems:the final explanations are highly dependent on the models and data sets they test(Atanasova et al., 2020; Ding and Koehn, 2021), and they can be easily fooled whenmodels do not faithfully show their predictions (Slack et al., 2020; Sinha et al., 2021).\n1.4 Current explanability reserachStudies focusing on model explanations with post-hoc attribution methods can be furthercategorized into two directions: 1) Designing or analyzing explanation methods ondownstream tasks (Hao et al., 2021; Ding and Koehn, 2021). 2) Incorporating explanationsprovided by attribution methods (or human-provided explanations) into modeltraining to enhance model performance and explanation quality (Zaidan et al., 2007;Atanasova et al., 2022).\n2 Motivations and Goals of this studyMotivation: To address low-resources settingsCurrent reserach heavily on the models trained/fine-tuned with rich datasets. However, not all tasks can be approachedby fine-tuning a pre-trained model or training a model from scratch. Moreover, obtaining annotated explanations is challenging both time-wise and resource-wise.\nConsidering low-resource settings where training data is scarce, it is challenging to developfine-tuned, or train-from-scratch models to perform downstream tasks, let aloneprovide explanations for predictions. We think low-resource settings are worth payingattention to and exploring because 1) principle perspective: social responsibility reasons (Ruder, 2020). We also want explanations in these settings. 2) practical perspective: explanations help understand models, by analyzing/incorporating explanations (into modells), we might gain more insights about the task and thus improve task performance, given limited data. \napproach interpretability issues in low-resource settings, we propose combining promptbasedmodels (PBMs) with explanation methods. \nChallenge2: lacking comparisions between attention and other post-hoc explanation methods with PBMBesides the problem of lacking focus on low-resource settings in interpretability research,another gap we notice is that there are rarely comprehensive comparisons between attentionand other attribution methods in terms of the explanations they give. Though someworks involve attention in the model‚Äôs interpretability (Sydorova et al., 2019; Pruthiet al., 2022), they either focus on only one facet of the comparison or utilize the finetunedmodels to derive explanations. We aim to compare explanations extracted byattention and other attribution methods in both PBM and fine-tuned model settings.We think such comparisons are crucial for providing direct observations of attentionperformance against other methods and enabling more understanding of explanationmethods under different setups.\nReferences[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pretrainingof deep bidirectional transformers for language understanding. In Proceedingsof the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Longand Short Papers), pages 4171‚Äì4186, Minneapolis, Minnesota, June 2019. Associationfor Computational Linguistics. \n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, PrafullaDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, RewonChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, ChristopherHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and DarioAmodei. Language models are few-shot learners. In Proceedings of the 34th InternationalConference on Neural Information Processing Systems, NIPS‚Äô20, Red Hook,NY, USA, 2020. Curran Associates Inc. \n[3] Andreas Madsen, Siva Reddy, and Sarath Chandar. Post-hoc interpretability for neuralnlp: A survey. ACM Comput. Surv., jul 2022.\n[4] Finale Doshi-Velez and Been Kim. Considerations for Evaluation and Generalization inInterpretable Machine Learning, pages 3‚Äì17. Springer International Publishing, Cham,2018.\n[5] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions.In Proceedings of the 31st International Conference on Neural Information ProcessingSystems, NIPS‚Äô17, page 4768‚Äì4777, Red Hook, NY, USA, 2017. Curran AssociatesInc.\n[6] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ‚Äùwhy should i trust you?‚Äù:Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining, 2016.\nWhat Do Vision and Language Models NOT learnThis is a summary of the project from the course Lanugage and Vision at the University of Stuttgart.\nUPDATE: our paper is accepted in the student session at KONVENS 2022\nContents\nIntro to pretrained LV models\nIntro to this project\nMethodology\nExperiments and results\nFuture work\n\n1. Intro to pretrained LV modelsgood resources to start:\nhttps://lilianweng.github.io/posts/2022-06-09-vlm/\nhttps://arxiv.org/pdf/2202.10936.pdf\n2. Intro to this projectPretrained LV models seems to work well on various tasks, but blindly applying them without understanding what knowledge they actually learn is dangerous.\n Therefore, we try to answer the question: what do the pretrained LV models actually learn?  Specifically, we want to know if LV models encode information about:  color, number, nouns, verbs and syntax. \n3. MethodologyThe main idea of our method is that  changing key information in a caption should confuse the models when the models attend to both the visual and texual information. \nFor instance, the below caption and image are perfectly aligned. But if I change the word ‚Äúcat‚Äù in the caption into ‚Äúdog‚Äù. You immediately notice the image and caption do not align any more, because you have the knowledge of what a cat looks like.\n\n\n\n\n\nCaption: A cat is playing with a ball.\n\n\n\nBased on this idea, we create probes (‚Äúdistorted‚Äù captions),  feed them into the models with images and compare model‚Äôs confidence in the alignment between the image and captions of the original and probe cases. In the following, I will explain:\n\nwhat are these probes and how to create them?\nwhich dataset and models did we use?\nwhat does the ‚Äúmodel‚Äôs confidence‚Äù mean? how to evaluate that?\n\n3.1 Creating probes3.1.1 ColorGoal:\n\nAre the models able to tell whether a color property of an object in the caption matches the color of the respective object in the image?\nWhen the models fail to detect the mismatch, is this caused by the learned priors (dataset or linguistic priors)?\n\nHow:\n\nblind sampled colors: blind sampling is done by sampling from a predefined set of colors randomly, and substituting the original color in the caption with the sampled one.\ncontrolled sampled colors: controlled sampling is analogous to the blind sampling, however, here, we aggregate all the color attributes ofthe objects in the dataset to create a plausible set of colors that each object can have.\n\nIn the caption generation, we then sample from the respective set of plausible colors of the object, and substitute the original color with this sampled one.\n3.1.2 NumberGoal: examining models‚Äô ability to exact counting of objects.\nHow: we do not probe for groups of objects of unspecified numbers (e.g. many, several,multiple). The caption generation of number probes follow analogously to that of the blind color probes.\n3.1.3 NounsGoal\n\ndetermine whether a model is able to detect that a noun in the caption does not match the respective object in the image.\ndetermine whether a model has a tendency of preferring a more or a less general concept.\n\nHow\n\nNoun probing:  in each caption we randomly select one of the nouns and replace it with another noun, randomly chosen out of 10 most similar words according to a Gensim model. For instance, donut is replaced with bagel, and car is replaced with motorcycle.\nwe replace a random noun in a caption with its hypernym from WordNet: dog is replaced with domesticated animal, and hat is replaced with headgear. Important to note here is that such a replacement might make a caption less specific, however does not create a mismatch between the image and the caption.\n\n3.1.4 VerbsGoal:  test models‚Äô ability of correctly identifying verbs denoting actions with two types of replacement: physical and relational.\nHow\n\nphysical probes: we replace a verb with its semantically similar counterpart.\nrelational probes: we extract the subject-verb-object (SVO) pair from a caption and replace the verb with a semantically different verb that fits in the SVO template: &lt;girl, climbing, stairs&gt; is changed to &lt;girl, cleaning, stairs&gt;.3.1.5. Syntax\n\nGoal\n\nIs the compositionality of concepts preserved in the pretrained models, or are the models learning concepts non-compositionally: e.g. is wooden building treated as a combined concept or as two separate concepts (wooden and building)?\nIs there an observable tendency of preferring more/less detailed descriptions: e.g. does removing wooden from wooden building result in a significant change in the perceived similarity of the image and caption pair?\n\nHow\nwe select captions that contain two nouns (objects in the image) with at least one of them having an adjective modifier (property of the object), and create probes for each qualified caption by swapping their properties (adj-swap). To answer the second question, we create probes by removing the adjective modifiers of a noun (no-adj).\n3.2 Dataset and models\nFlickr8k dataset consisting of 8,000 images that are eachpaired with five different captions\nCLIP and VisualBert\n\n3.3 Evaluationswe extract the image-text similarity scores for the label 1 (match case) from the models for the original and modified captions, and feed these into a softmax classifier to obtain our results. \nWe report on the accuracies of these preferences and the averaged similarity scores. \n4. Result and implications\n\n\nProbe\nnum instance\nCLIP\nVisualBERT\n\n\n\nctrl color\n3120\n0.80\n0.59\n\n\nblind color\n14255\n0.86\n0.54\n\n\nnumber\n8138\n0.85\n0.72\n\n\nsyntax adj-swap\n4711\n0.68\n0.56\n\n\nsyntax no-adj\n4711\n0.79\n0.43\n\n\nnoun\n39856\n0.80\n0.58\n\n\nhypernym\n40283\n0.69\n0.59\n\n\nverb physical\n107\n0.71\n0.62\n\n\nverb relational\n107\n0.89\n0.59\n\n\n\nCLIP performs much better than Visualbert\nas expected, blind color is easier than ctrl color, there might be a learned prior of color and nouns.\nwe could imagine that swapping adj should be easier than deleting adj, because the latter case does not necessarily result in wrong captions. However, we found CLIP thinks the opposite. This might suggest having attributes is more important for matching image and caption than attaching the right adj to the right nounds. The model might still learn concept individually.\nas expected, hypernym is more difficult.\n\n5. Future works\nincorporate more models\n\n","categories":["project"],"tags":["prompting","explainability","low resources"]},{"title":"Analysis and Applications of Explanatory Signals from Prompt-Based Models","url":"/2022/12/29/thesis/","content":"LEFT TO DO\nfixing image issues\nadding hyperlink\nadding summarizd version of results, perhaps add figures accordingly.\nmaking reference more compact.\n\nAbstractExplanatory signals, such as gradients, can provide information about the importanceof different input parts for the output of models, thus improving models‚Äô interpretability.In natural language processing (NLP), they reveal how different parts of texts, e.g.,tokens, contribute to a prediction from black-box models, e.g., deep neural networks.Current research on interpretability focuses on fine-tuned models: either extracting explanatorysignals from models for analysis or injecting them into models to improve taskperformance or explanation quality. This poses a challenge to interpretability in lowresourcesettings, where data are scarce to fine-tune models. We approach this challengeby leveraging prompt-based models (PBMs) to provide explanations. A comprehensivecomparison between explanations obtained from PBMs and fine-tuned models isconducted in various settings. More specifically, we explore the influence of trainingsize, prompting strategies, and explanation methods on the output explanations fromthe perspective of plausibility, faithfulness, and a proxy task. The tasks and data setswe explore are sentiment classification (Tweet Sentiment Extraction) and natural languageinference (e-SNLI). We find that PBMs generate more plausible explanations thanfine-tuned models in the low-resource setting. Additionally, among the three explanationmethods we examine, namely, attention, Integrated Gradients, and Shapley ValueSampling, we observe Shapley Value Sampling consistently outperforms other methodsin plausibility and faithfulness. Apart from providing comparisons in various aspects,we propose a novel method to improve task performance with explanations by enrichingthe prompts of PBMs with explanatory signals. Our method achieves up to 3% ofperformance gains compared with PBMs without explanations.\n1. Introduction1.1 BackgroundThe development of NLP models has brought notable improvements to various downstream tasks [1, 2]. However, it comes at the expense of interpretability: as neural models become larger and more complex, it is difficult to disentangle their structures and explain how they generate their outputs. This is detrimental to fields where high-stakes decisions are made, such as criminal justice and finance [3]. \nIn this work, we follow the definition from [4],that interpretability is ‚Äúthe ability [of a model] to explain or to present [its predictions]in understandable terms to a human‚Äù. We treat explainability as the synonym of interpretability and use them interchangeably.\nGiven the status quo and why we need explanability in NLP, we move on to introduce the taxonomy of explanation methods and some common explantion methods. \n1.2 Taxonomy of explaination methodsLocal vs. Global. One vital dimension to categorizing explanation methods is whether themethods target interpreting specific instances or the whole model/reasoning mechanismin general. The former includes SHAP ([5]), and Lime ([6] while the latter can be exemplified by decision trees.\nPost-hoc vs. Self-explaining. This dimension groups explanation methods by how theyare derived, i.e., applying post-hoc operations on a model or during the model‚Äôs predictionprocess. The former technique is often model-agnostic, while the latter is deeplyintegrated with the model‚Äôs working mechanism, exemplified by attention.\nFree texts vs. Token saliency. Another dimension distinguishes the way explanationsare shown: either by generating free texts as explanations or showing input importance, mostly tokens, by highlighting.\n\n","categories":["project"],"tags":["prompting","explainability","low resources"]}]