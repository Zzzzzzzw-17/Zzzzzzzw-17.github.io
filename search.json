[{"title":"Enriched Attention with Entity Masking","url":"/2022/04/27/relation_extraction/","content":"Enriched Attention with Entity MaskingThis is a summary of a relation classification task with enriched attention. Please refer to this paper for more information.\nContents covered are listed as follows: \n\nIntro to the relation extraction task \nRelated works\nIntro to Meta-Embeddings\nExperiments\nResults\nTake-away-messages\n\n1. Relation extraction1.1 Task definition: \nGiven a sentence and two arguments, predict the relationship of the two arguments. \n\nE.g., Barack Obama Sr., the father of Barack Obama, was born is1936 and married his first wife Kezia at the age of 18.\n\nIn this sentence, we can have four arguments and many relation pairs: Barack Obama Sr. and Barack Obama have the relationship of is_father_of, Barack Obama Sr. and 1936 can have the relationship of born_in and Barack Obama Sr. and Kezia the relationship husband_and_wife. These relationships are defined by researchers.\n1.2 Usage\nknowledge graph population: converting unstructured data in structured data.For instance, if we have a document, we can identity all potential arguments (NER detection) and get their relationship. We can append the relation information into the knowledge graph\n1.3 Challenges\nSentence can be long, the distance between intersted arguments can be long, there can be distractors between the intersted arguments.\n1.4 Datasets\nWe will focus on the Tacred dataset only in this post.\n2. Related work\nEnriched attention\nTACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task\nRelation Classification with Entity Type Restriction\nAn Improved Baseline for Sentence-level Relation Extractiontake-away-messages: entity nanmes hepls/ better ways to encode entity information into the texts/ unmasking does not lead to overfitting\n A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models\n\n3. Meta-embeddingsThe basic idea is that we can combine different types of embeddings to have better representations for tokens. More information can be found in this paper.\n4. Experiments Here is a summary table of the configurations that I tried. You can find the explanations below.\n\n\n\nembeddings\nencoder\nmasking\nattention\nadditional_features\nnote\n\n\n\nglove/bpe/transformer/meta\nbi-lstm\nentity mark\ntrue / false\nglobal, local dependency, entity types\ndefault\n\n\nglove/bpe/transformer/meta\nbi-lstm\nall entity masking strategy except entity mark\nfalse\nnone\nchoose masking strategy\n\n\ntransformer\ntransformer\nall-masking\nfalse\nnone\ntransformer_only\n\n\nExplanations\nEmbeddings: different embeddings were tested, namely glove-300d, bpe embeddings, transformer embeddings (roberta and xlm-roberta) and combinations of them (suggested by the option meta). \n\nEncoder: two types of encoder were examined, namely bi-lstm encoder and transformer encoder. The former is from this paper  and the latter this .\n\nMasking: five different entity masking strategies were tested. They are:\n  üéÄ The original sentence is: Bill was born in Seattle. \n\nEntity mask: [SUBJ-PERSON] was born in [OBJ-CITY]. \n\nEntity marker:  [E1] Bill [/E1] was born in [E2] Seattle [/E2]. \n\nEntity marker (punct):  @ Bill @ was born in # Seattle #. \n\nTyped entity marker: „ÄàS:PERSON„Äâ Bill „Äà/S:PERSON„Äâ was born in „ÄàO:CITY„Äâ Seattle „Äà/O:CITY„Äâ. \n\nTyped entity marker (punct): @ * person * Bill @ was born in # ‚àß city ‚àß Seattle #. \n\n\n\nAttention and additional features: these are exclusive to the lstm encoder. For more information, please refer to  this paper .\n\n\n5. Results on Tacred datasetNotation\n\ndefault : BILSTM model, no attention, no additional features, use only GLOVE embeddings.\ndefault + xx: additional settings added on the default mode.\ndefault * xx : change element in default setting, for instance, embedding method .\ntransformer: using roberta-base model as encoder, no additional features, use entity mask.\n\n\n\n\nsetting\nmodel\ndev f1\ntest f1\nsetting details\n\n\n\n1\ndefault\n0.6473\n0.6199\n\n\n\n2\ndefault + attn\n0.6455\n0.6414\nwith attn=true, global, local and entity info\n\n\n3\ndefault * masking\n0.6208\n0.5977\nchange masking from entity mask to type entity marker punct\n\n\n4\ndefault * embeddings + attn\n0.6540\n0.6620\n{emb: bpe and GLOVE}, {attn: true}, {local, global, entity type: true}\n\n\n5\ndefault * embeddings\n0.6544\n0.6388\n{emb: bpe and GLOVE}\n\n\n6\ndefault * embeddings * masking\n0.6295\n0.5951\n{emb: bpe and GLOVE}, {masking: entity marker}\n\n\n7\ndefault * embeddings * masking\n0.6201\n0.5946\n{emb: bpe and GLOVE}, {masking: entity marker (punct)}\n\n\n8\ndefault * embeddings * masking\n0.6545\n0.6329\n{emb: bpe and GLOVE}, {masking: typed entity marker}\n\n\n9\ndefault * embeddings * masking\n0.6566\n0.6397\n{emb: bpe and GLOVE}, {masking: typed entity marker (punct)}\n\n\n10\ntransformer\n0.6971\n0.7030\n\n\n\n11\ntransformer * masking\n0.7113\n0.7063\nchange masking from entity mask to type entity marker punct\n\n\n6. Take-away-messages\ntransformer encoder performs better than bi-lstm.\ncomparing different masking strategies (setting 5-9),  we may conclude incorporating entity type information into text is beneficial.\nattention and additional features (setting1-2) helps lstm encoder for this task.\na combination of bpe and GLOVE embedding seems to work better than using GLOVE only. (setting1 and setting5)\n\n\nThis work is fully supported by Bosch. \n","categories":["work"],"tags":["embeddings","relation classification"]},{"title":"Hexo error archive","url":"/2022/04/27/error-archive/","content":"\nWhen compiling markdown file\n\nerr: YAMLException: end of the stream or a document separator is expected at line 6, column 39:\n\n\n\nI browse through many blogs, some say it is a problem of not using space property, i.e., you should add an empty space   after :.But this is not the problem for my case, the problem is I did not add the title format into my markdown file.\n\n\ntitle: Hexo error archivedate: 2022-04-27 12:48:43tags:\n\n\n"},{"title":"Probing into Language and Vision model","url":"/2022/04/27/vision/","content":"What Do Vision and Language Models NOT learnThis is a summary of the project from the course Lanugage and Vision at the University of Stuttgart.\nUPDATE: our paper is accepted in the student session at KONVENS 2022\nContents\nIntro to pretrained LV models\nIntro to this project\nMethodology\nExperiments and results\nFuture work\n\n1. Intro to pretrained LV modelsgood resources to start:\nhttps://lilianweng.github.io/posts/2022-06-09-vlm/\nhttps://arxiv.org/pdf/2202.10936.pdf\n2. Intro to this projectPretrained LV models seems to work well on various tasks, but blindly applying them without understanding what knowledge they actually learn is dangerous.\n Therefore, we try to answer the question: what do the pretrained LV models actually learn?  Specifically, we want to know if LV models encode information about:  color, number, nouns, verbs and syntax. \n3. MethodologyThe main idea of our method is that  changing key information in a caption should confuse the models when the models attend to both the visual and texual information. \nFor instance, the below caption and image are perfectly aligned. But if I change the word ‚Äúcat‚Äù in the caption into ‚Äúdog‚Äù. You immediately notice the image and caption do not align any more, because you have the knowledge of what a cat looks like.\n\n\n\n\n\nCaption: A cat is playing with a ball.\n\n\n\nBased on this idea, we create probes (‚Äúdistorted‚Äù captions),  feed them into the models with images and compare model‚Äôs confidence in the alignment between the image and captions of the original and probe cases. In the following, I will explain:\n\nwhat are these probes and how to create them?\nwhich dataset and models did we use?\nwhat does the ‚Äúmodel‚Äôs confidence‚Äù mean? how to evaluate that?\n\n3.1 Creating probes3.1.1 ColorGoal:\n\nAre the models able to tell whether a color property of an object in the caption matches the color of the respective object in the image?\nWhen the models fail to detect the mismatch, is this caused by the learned priors (dataset or linguistic priors)?\n\nHow:\n\nblind sampled colors: blind sampling is done by sampling from a predefined set of colors randomly, and substituting the original color in the caption with the sampled one.\ncontrolled sampled colors: controlled sampling is analogous to the blind sampling, however, here, we aggregate all the color attributes ofthe objects in the dataset to create a plausible set of colors that each object can have.\n\nIn the caption generation, we then sample from the respective set of plausible colors of the object, and substitute the original color with this sampled one.\n3.1.2 NumberGoal: examining models‚Äô ability to exact counting of objects.\nHow: we do not probe for groups of objects of unspecified numbers (e.g. many, several,multiple). The caption generation of number probes follow analogously to that of the blind color probes.\n3.1.3 NounsGoal\n\ndetermine whether a model is able to detect that a noun in the caption does not match the respective object in the image.\ndetermine whether a model has a tendency of preferring a more or a less general concept.\n\nHow\n\nNoun probing:  in each caption we randomly select one of the nouns and replace it with another noun, randomly chosen out of 10 most similar words according to a Gensim model. For instance, donut is replaced with bagel, and car is replaced with motorcycle.\nwe replace a random noun in a caption with its hypernym from WordNet: dog is replaced with domesticated animal, and hat is replaced with headgear. Important to note here is that such a replacement might make a caption less specific, however does not create a mismatch between the image and the caption.\n\n3.1.4 VerbsGoal:  test models‚Äô ability of correctly identifying verbs denoting actions with two types of replacement: physical and relational.\nHow\n\nphysical probes: we replace a verb with its semantically similar counterpart.\nrelational probes: we extract the subject-verb-object (SVO) pair from a caption and replace the verb with a semantically different verb that fits in the SVO template: &lt;girl, climbing, stairs&gt; is changed to &lt;girl, cleaning, stairs&gt;.3.1.5. Syntax\n\nGoal\n\nIs the compositionality of concepts preserved in the pretrained models, or are the models learning concepts non-compositionally: e.g. is wooden building treated as a combined concept or as two separate concepts (wooden and building)?\nIs there an observable tendency of preferring more/less detailed descriptions: e.g. does removing wooden from wooden building result in a significant change in the perceived similarity of the image and caption pair?\n\nHow\nwe select captions that contain two nouns (objects in the image) with at least one of them having an adjective modifier (property of the object), and create probes for each qualified caption by swapping their properties (adj-swap). To answer the second question, we create probes by removing the adjective modifiers of a noun (no-adj).\n3.2 Dataset and models\nFlickr8k dataset consisting of 8,000 images that are eachpaired with five different captions\nCLIP and VisualBert\n\n3.3 Evaluationswe extract the image-text similarity scores for the label 1 (match case) from the models for the original and modified captions, and feed these into a softmax classifier to obtain our results. \nWe report on the accuracies of these preferences and the averaged similarity scores. \n4. Result and implications\n\n\nProbe\nnum instance\nCLIP\nVisualBERT\n\n\n\nctrl color\n3120\n0.80\n0.59\n\n\nblind color\n14255\n0.86\n0.54\n\n\nnumber\n8138\n0.85\n0.72\n\n\nsyntax adj-swap\n4711\n0.68\n0.56\n\n\nsyntax no-adj\n4711\n0.79\n0.43\n\n\nnoun\n39856\n0.80\n0.58\n\n\nhypernym\n40283\n0.69\n0.59\n\n\nverb physical\n107\n0.71\n0.62\n\n\nverb relational\n107\n0.89\n0.59\n\n\n\nCLIP performs much better than Visualbert\nas expected, blind color is easier than ctrl color, there might be a learned prior of color and nouns.\nwe could imagine that swapping adj should be easier than deleting adj, because the latter case does not necessarily result in wrong captions. However, we found CLIP thinks the opposite. This might suggest having attributes is more important for matching image and caption than attaching the right adj to the right nounds. The model might still learn concept individually.\nas expected, hypernym is more difficult.\n\n5. Future works\nincorporate more models\n\n","categories":["project"],"tags":["language and vision","probing","embeddings"]}]